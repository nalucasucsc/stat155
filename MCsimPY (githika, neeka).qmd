---
title: "Monte Carlo Simulation: Comparing Linear, Ridge, and Lasso Regression"
format: pdf
jupyter: python3
---
## Group Members: Githika Annapureddy, Neeka Lucas

## Our Changes
We changed the number of data sets generated from 10, to 30. Generating more data sets increases the accuracy if this experiment. We also expanded the data set to include variable: alpha, or penalty. In the final table, we are comparing the 3 different alpha's (0.5, 0.1, 0.15) to see, which out of the three, produces values that closely "match" the true values.

We will simulate 90 datasets in total:
- 30 datasets with no correlation between predictors $x_1$ and $x_2$
- 30 datasets with mild correlation
- 30 datasets with high correlation

## Our Results
Our results are not statistically significant. Changing the alpha or penalty value for Lasso and Ridge did not significantly change the Mean MSE or Mean SD. The benefit of finding a more optimal penalty is diminished when there are only two features. Lasso and Ridge would perform better under a more optimal penalty if there were many features. 

Furthermore, increasing the number of datasets for each type of correlation from 10 to 30 did not create a statistically significant change. The resulting plot has outliers, but the spread for each correlation is in a similar range as the original plot (0.8 to 1.8). 

## Our Figures
![Final Plot](/Users/githika/Downloads/plot.png)
         Model Correlation  Mean_MSE    SD_MSE
0   Lasso_0.05   High Corr  1.015201  0.297364
1   Lasso_0.05   Mild Corr  1.066211  0.294353
2   Lasso_0.05     No Corr  1.119908  0.261960
3    Lasso_0.1   High Corr  1.022376  0.295650
4    Lasso_0.1   Mild Corr  1.071319  0.292050
5    Lasso_0.1     No Corr  1.124705  0.264276
6   Lasso_0.15   High Corr  1.035773  0.296964
7   Lasso_0.15   Mild Corr  1.081785  0.290030
8   Lasso_0.15     No Corr  1.140281  0.269551
9       Linear   High Corr  1.030851  0.292592
10      Linear   Mild Corr  1.068285  0.294628
11      Linear     No Corr  1.122863  0.258875
12  Ridge_0.05   High Corr  1.027091  0.295441
13  Ridge_0.05   Mild Corr  1.068080  0.294034
14  Ridge_0.05     No Corr  1.123153  0.259027
15   Ridge_0.1   High Corr  1.024776  0.298250
16   Ridge_0.1   Mild Corr  1.067900  0.293449
17   Ridge_0.1     No Corr  1.123458  0.259183
18  Ridge_0.15   High Corr  1.023444  0.300896
19  Ridge_0.15   Mild Corr  1.067746  0.292874
20  Ridge_0.15     No Corr  1.123779  0.259343


## Objective

This Monte Carlo simulation compares the performance of three regression models:
- Linear Regression
- Ridge Regression
- Lasso Regression

The models will be evaluated on Mean Squared Error (MSE). The results will be summarized using the mean and standard deviation of the MSE for each combination of model and correlation structure.

## Setup

```{python}
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt

np.random.seed(116)
```


## Data Generation Function

```{python}
def generate_data(n=100, correlation=0.0):
    mean = [0, 0]
    cov = [[1, correlation], [correlation, 1]]
    X = np.random.multivariate_normal(mean, cov, size=n)
    x1, x2 = X[:, 0], X[:, 1]
    noise = np.random.normal(0, 1, size=n)
    y = 0 * x1 + 4 * x2 + noise
    return pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})
```

## Simulation Loop

```{python}
results = []
correlations = [("No Corr", 0.0), ("Mild Corr", 0.5), ("High Corr", 0.99)]
models = {
    "Linear": LinearRegression(),
    "Ridge_0.1": Ridge(alpha=0.1),
    "Lasso_0.1": Lasso(alpha=0.1),
    "Ridge_0.05": Ridge(alpha=0.05),
    "Lasso_0.05": Lasso(alpha=0.05),
    "Ridge_0.15": Ridge(alpha=0.15),
    "Lasso_0.15": Lasso(alpha=0.15)
}

for label, rho in correlations:
    for i in range(30): # changed to 30
        df = generate_data(correlation=rho)
        X = df[['x1', 'x2']]
        y = df['y']
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i)

        for model_name, model in models.items():
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            mse = mean_squared_error(y_test, y_pred)
            results.append({
                'Model': model_name,
                'Correlation': label,
                'MSE': mse
            })
```

## Results Summary

```{python}
results_df = pd.DataFrame(results)
summary = results_df.groupby(['Model', 'Correlation']).agg(
    Mean_MSE=('MSE', 'mean'),
    SD_MSE=('MSE', 'std')
).reset_index()

summary
```


## Visualization (Optional)

```{python}
sns.boxplot(data=results_df, x='Correlation', y='MSE', hue='Model')
plt.title("Model MSE by Predictor Correlation")
plt.show()
```

